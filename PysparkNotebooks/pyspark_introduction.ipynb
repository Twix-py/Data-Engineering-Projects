{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is a data processing framework designed for working with large datasets that don't fit into memory. At its core, Spark uses RDDs (Resiliant Distributed Dataset) to offer an interface for users to interact and perform transformations with their data. Spark offers a number of APIs for using the framework, such as Rspark for R, a Java API, and Pyspark - the API we will be using in this Notebook\n",
    "\n",
    "First we will import the library and instantiate our Spark application to begin working with our data. We will also specify a path to a CSV file which we can begin to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "PATH = r'/home/tom/Documents/csv_files/pokemon.csv'\n",
    "spark = SparkSession.builder.appName('pyspark_introduction').config(\"spark.ui.port\", \"4050\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our Spark driver and specified a file path to our CSV file, we can load in the dataset and begin to explore it using the .read.csv() method. We will also specify a schema for our dataset to tell it what data types and columns to expect - the inferSchema parameter can be passed to allow Spark to automatically infer the schema - but this isn't always reliable and is prone to error, so it is best practice to specify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"pokemon_id INTEGER, name STRING, type_1 STRING, type_2 STRING,\n",
    "total INTEGER, HP INTEGER, attack INTEGER, defense INTEGER, special_attack INTEGER, special_defense INTEGER, \n",
    "speed INTEGER, generation INTEGER, legendary BOOLEAN\"\"\"\n",
    "\n",
    "df = spark.read.csv(PATH, schema = schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform some basic transformation and analytics on our dataset. We will drop the total column and implement it ourselves to show how the .withColumn() method works, as well as dropping any NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                name|total|\n",
      "+--------------------+-----+\n",
      "|                Name| null|\n",
      "|           Bulbasaur|  318|\n",
      "|             Ivysaur|  405|\n",
      "|            Venusaur|  525|\n",
      "|VenusaurMega Venu...|  625|\n",
      "|          Charmander|  309|\n",
      "|          Charmeleon|  405|\n",
      "|           Charizard|  534|\n",
      "|CharizardMega Cha...|  634|\n",
      "|CharizardMega Cha...|  634|\n",
      "|            Squirtle|  314|\n",
      "|           Wartortle|  405|\n",
      "|           Blastoise|  530|\n",
      "|BlastoiseMega Bla...|  630|\n",
      "|            Caterpie|  195|\n",
      "|             Metapod|  205|\n",
      "|          Butterfree|  395|\n",
      "|              Weedle|  195|\n",
      "|              Kakuna|  205|\n",
      "|            Beedrill|  395|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop('total')\n",
    "df = df.withColumn('total', df.attack + df.defense + df.special_attack + df.special_defense + df.speed + df.HP)\n",
    "df.select(\"name\", \"total\").show() #use the .select() method to select individual columns and the .show() to\n",
    "#show dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand how to apply some basic transformations on our dataset, we can begin to experiment with the vast ammount of functions that the Pyspark SQL module contains to extract some information on our dataset and perform some more transformations.\n",
    "\n",
    "We will import the pyspark.sql.functions module which contains numerous functions that can be applied to DataFrames. We will replace all the columns where the type_1 column is equal to Bug to Stone using the .regexp_replace() method, as well as apply some aggregate functions and groupings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|  type_1|       average_hp|\n",
      "+--------+-----------------+\n",
      "|   Water|          72.0625|\n",
      "|  Poison|            67.25|\n",
      "|   Steel|65.22222222222223|\n",
      "|    Rock|65.36363636363636|\n",
      "|     Ice|             72.0|\n",
      "|   Ghost|          64.4375|\n",
      "|   Fairy|74.11764705882354|\n",
      "| Psychic|70.63157894736842|\n",
      "|  Dragon|          83.3125|\n",
      "|  Flying|            70.75|\n",
      "|Electric|59.79545454545455|\n",
      "|    Fire|69.90384615384616|\n",
      "|   Stone|56.88405797101449|\n",
      "|  Type 1|             null|\n",
      "|  Ground|         73.78125|\n",
      "|    Dark|66.80645161290323|\n",
      "|Fighting|69.85185185185185|\n",
      "|   Grass|67.27142857142857|\n",
      "|  Normal|77.27551020408163|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as sql\n",
    "df = df.withColumn('type_1', sql.regexp_replace('type_1', 'Bug', 'Stone').alias('type_1'))\n",
    "groups = df.groupBy('type_1')\n",
    "aggregate_data = groups.agg({'HP': 'mean'})\n",
    "aggregate_data = aggregate_data.withColumnRenamed('avg(HP)', 'average_hp') #renaming column\n",
    "aggregate_data.show() #returns a DataFrame with the average HP value per type_1 value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will export the aggregate data our local machine for use. Spark offers methods for exporting to a CSV file, RDBMs using JDBC drivers, as well as a parquet file - which we will be doing using the .write.parquet() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('/home/tom/Documents/csv_files/pokemon_parquet.parquet') #takes a file path to save as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we learnt how to load a CSV file into Spark using the .read.csv() method, how to specify a schema for our dataset, apply some basic transformations such as dropping NULL values and aggregate functions - and finally exporting our DataFrame as a parquet file to our local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
